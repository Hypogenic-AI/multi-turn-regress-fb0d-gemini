\begin{abstract}
Large Language Models (LLMs) are increasingly deployed in long-context applications, such as persistent agents and interactive role-playing systems. However, their ability to maintain specific behavioral constraints and personas over extended interactions remains under-explored. In this work, we investigate the hypothesis that LLMs regress to their base-level priors---specifically, the ``helpful assistant'' persona---as conversation length increases, overriding session-specific instructions. We conduct a 50-turn simulated conversation using 	\texttt{gpt-4o-mini}, enforcing a strict linguistic constraint (avoiding common stop words) and a distinct pirate persona. Our results reveal a strong positive correlation ($r=0.568$) between turn count and constraint violations, alongside a steady increase in regression to default assistant behavior ($r=0.446$) and a decline in persona fidelity ($r=-0.300$). These findings quantify ``alignment decay,'' suggesting that current alignment techniques produce fragile behavioral states that erode as the context window fills with model-generated content. We discuss the implications for designing robust long-context agents and potential mitigation strategies.
\end{abstract}
