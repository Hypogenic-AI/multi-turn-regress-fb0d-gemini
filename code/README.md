# Cloned Repositories

This directory contains the codebases for benchmarks and frameworks relevant to multi-turn LLM evaluation.

| Repo | URL | Purpose |
|------|-----|---------|
| `mt-bench-101` | [mtbench101/mt-bench-101](https://github.com/mtbench101/mt-bench-101) | Fine-grained benchmark for multi-turn dialogues. |
| `Multi-IF` | [facebookresearch/Multi-IF](https://github.com/facebookresearch/Multi-IF) | Multi-turn instruction following benchmark. |
| `RAD-Bench` | [mtkresearch/RAD-Bench](https://github.com/mtkresearch/RAD-Bench) | Retrieval Augmented Dialogue benchmark. |
| `MT-Eval` | [KwanWaiChung/MT-Eval](https://github.com/KwanWaiChung/MT-Eval) | Multi-turn capabilities evaluation. |
| `persistent-personas` | [peluz/persistent-personas](https://github.com/peluz/persistent-personas) | Code for the "Persistent Personas?" paper experiments. |
| `lost_in_conversation` | [microsoft/lost_in_conversation](https://github.com/microsoft/lost_in_conversation) | Code for the "LLMs Get Lost in Multi-Turn Conversation" paper. |

## Usage Notes
- Most repositories require their own dependency setup.
- `persistent-personas` and `lost_in_conversation` are the most directly relevant for testing the "regression to prior" hypothesis.
